{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNYDZg4VUcBhEBTRj7KKXTP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodRock/deep-learning-goodfellow/blob/main/chapter_7_regularization_for_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7 | Regularization for Deep Learning\n",
        "\n",
        "References:\n",
        "1. Hinton, G. E. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.\n",
        "2. Srebro, N., & Shraibman, A. (2005, June). Rank, trace-norm and max-norm. In International conference on computational learning theory (pp. 545-560). Springer Berlin Heidelberg.\n",
        "3. Sietsma, J., & Dow, R. J. (1991). Creating artificial neural networks that generalize. Neural networks, 4(1), 67-79.\n",
        "4. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1), 1929-1958.\n",
        "5. Jim, K. C., Giles, C. L., & Horne, B. G. (1996). An analysis of noise in recurrent neural networks: convergence and generalization. IEEE Transactions on neural networks, 7(6), 1424-1438.\n",
        "6. Hochreiter, S., & Schmidhuber, J. (1994). Simplifying neural nets by discovering flat minima. Advances in neural information processing systems, 7.\n",
        "7. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2818-2826).\n",
        "\n",
        "\n",
        "We defined regularization as \"any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error\"."
      ],
      "metadata": {
        "id": "oHF0W8NFHiVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_THe7whOHBvn",
        "outputId": "e1b8cd6f-a3be-45c6-f208-55e30bebc6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "I: 0 \t accuracy: 0.138 \t test accuracy: 0.296\n",
            "I: 10 \t accuracy: 0.609 \t test accuracy: 0.704\n",
            "I: 20 \t accuracy: 0.719 \t test accuracy: 0.72\n",
            "I: 30 \t accuracy: 0.745 \t test accuracy: 0.757\n",
            "I: 40 \t accuracy: 0.782 \t test accuracy: 0.763\n",
            "I: 50 \t accuracy: 0.8 \t test accuracy: 0.778\n",
            "I: 60 \t accuracy: 0.81 \t test accuracy: 0.783\n",
            "I: 70 \t accuracy: 0.816 \t test accuracy: 0.799\n",
            "I: 80 \t accuracy: 0.839 \t test accuracy: 0.799\n",
            "I: 90 \t accuracy: 0.856 \t test accuracy: 0.807\n",
            "I: 99 \t accuracy: 0.871 \t test accuracy: 0.806\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "\n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "\n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "\n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "\n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        "\n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "\n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "\n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if (self.creators is not None and\n",
        "               (self.all_children_grads_accounted_for() or\n",
        "                grad_origin is None)):\n",
        "\n",
        "                if (self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "\n",
        "                if (self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if (self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)\n",
        "\n",
        "                if (self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "\n",
        "                if (self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if (\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if (\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "                if (self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "\n",
        "                if (self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "\n",
        "                if (self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "\n",
        "                if (self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self.data.shape\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if (self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)\n",
        "\n",
        "    def sum(self, dim):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "\n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "\n",
        "        if (self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "\n",
        "    def transpose(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "\n",
        "        return Tensor(self.data.transpose())\n",
        "\n",
        "    def mm(self, x):\n",
        "        if (self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())\n",
        "\n",
        "    def sigmoid(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)), autograd=True, creators=[self], creation_op = \"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if (self.autograd):\n",
        "            return Tensor(np.tanh(self.data), autograd=True, creators=[self], creation_op = \"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "\n",
        "    def index_select(self, indices):\n",
        "        if (self.autograd):\n",
        "            new = Tensor(self.data[indices.data], autograd=True, creators=[self], creation_op = \"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "\n",
        "    def cross_entropy(self, target_indices):\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape)-1, keepdims=True)\n",
        "        target_dist = target.data\n",
        "        loss = -(np.log(softmax_output) * target_dist).sum(axis=1).mean()\n",
        "\n",
        "        if (self.autograd):\n",
        "            out = Tensor(loss, autograd=True, creators=[self], creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "\n",
        "class SGD(object):\n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "\n",
        "    def step(self, zero=True):\n",
        "        for p in self.parameters:\n",
        "            p.data -= p.grad.data * self.alpha\n",
        "\n",
        "            if (zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / (n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "\n",
        "        self.parameters.append(self.weight)\n",
        "        self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.mm(self.weight) + self.bias.expand(0, len(input.data))\n",
        "\n",
        "class Sequential(Layer):\n",
        "    def __init__(self, layers=list(), training=True):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.training = training\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                layer.training = self.training\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "class Dropout(Layer):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Only apply dropout when training.\n",
        "        if self.training:\n",
        "            # Multiply by 1 / (1 - p) to balance out the extra sensitivity.\n",
        "            self.mask = np.random.binomial(1, 1-self.p, input.shape) / (1-self.p)\n",
        "            return input * Tensor(self.mask, autograd=input.autograd)\n",
        "        return input\n",
        "\n",
        "    def backward(self, grad):\n",
        "        return grad * self.mask\n",
        "\n",
        "\n",
        "class MSELoss(Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        return ((pred - target) * (pred - target)).sum(0)\n",
        "\n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "\n",
        "class Relu(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.relu()\n",
        "\n",
        "\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train[0:1_000].reshape(1_000,28*28) / 255\n",
        "X_test = X_test[0:1_000].reshape(1_000,28*28) / 255\n",
        "y_train = y_train[0:1_000]\n",
        "y_test = y_test[0:1_000]\n",
        "\n",
        "# Onehot encoding\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test = np.eye(10)[y_test]\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.001\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "input_dim = 784\n",
        "hidden_size = 100\n",
        "output_dim = 10\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential([\n",
        "    Linear(input_dim,hidden_size),\n",
        "    Tanh(),\n",
        "    Dropout(p=0.5),\n",
        "    Linear(hidden_size, hidden_size),\n",
        "    Tanh(),\n",
        "    Dropout(p=0.5),\n",
        "    Linear(hidden_size, output_dim),\n",
        "    Sigmoid()\n",
        "])\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = SGD(model.get_parameters(), alpha)\n",
        "\n",
        "# Training loop\n",
        "for j in range(epochs):\n",
        "    correct_cnt = 0\n",
        "    model.train()\n",
        "    for i in range(int(len(X_train) / batch_size)):\n",
        "        batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
        "        input = Tensor(X_train[batch_start:batch_end], autograd=True)\n",
        "        target = Tensor(y_train[batch_start:batch_end], autograd=True)\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model.forward(input)\n",
        "\n",
        "        loss = criterion.forward(prediction, target)\n",
        "\n",
        "        for k, p in enumerate(prediction.data):\n",
        "            pred_label = p\n",
        "            true_label = y_train[batch_start+k: batch_start+k+1]\n",
        "            correct_cnt += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "\n",
        "        # Back propgation\n",
        "        loss.backward()\n",
        "        # Update the weights0\n",
        "        optimizer.step()\n",
        "\n",
        "    training_accuracy = correct_cnt / float(len(y_train))\n",
        "\n",
        "    if (j % 10 == 0 or j == epochs - 1):\n",
        "        model.eval()\n",
        "        test_correct_cnt = 0\n",
        "        for i in range(int(len(X_test) / batch_size)):\n",
        "          batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
        "          input = Tensor(X_test[batch_start:batch_end], autograd=True)\n",
        "          target = Tensor(y_test[batch_start:batch_end], autograd=True)\n",
        "\n",
        "          # Forward pass\n",
        "          prediction = model.forward(input)\n",
        "\n",
        "          for k, p in enumerate(prediction.data):\n",
        "              pred_label = p\n",
        "              true_label = y_test[batch_start+k: batch_start+k+1]\n",
        "              test_correct_cnt += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "\n",
        "        test_accuracy = test_correct_cnt / float(len(y_test))\n",
        "        print(f\"I: {j} \\t accuracy: {training_accuracy} \\t test accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # $ L^2$ Regularization - Ridge regresssion\n",
        "\n",
        "The $L^2$ parameters norm penalty common known as **weight decay**. This regularization strategy drives the weights closer to the origin, by adding a regularozation term $\\Omega (\\theta) = \\frac{1}{2} || w ||^2_2$ to the objective function. In other academic communities, $L^2$ reguliarzation is also known as **ridge regression** or **Tikhonoz regularization**.\n",
        "\n",
        "Such a model has the total objective function\n",
        "\n",
        "$$\n",
        "  \\tilde{J} (w;X,y) = \\frac{\\alpha}{2} w^\\top w + J(w;X,y)\n",
        "$$\n",
        "\n",
        "with the corresponding parameter gradient\n",
        "\n",
        "$$\n",
        "  \\nabla_w \\tilde{j} (W;X,y) = \\alpha w + \\nabla_w J(w;X,y)\n",
        "$$\n",
        "\n",
        "To take a single gradient step to update the weights, we perform this update\n",
        "\n",
        "$$\n",
        "  w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))\n",
        "$$\n",
        "\n",
        "Written another way, the update is\n",
        "\n",
        "$$\n",
        "  w \\leftarrow (1 - \\epsilon \\alpha)w - \\epsilon \\nabla_w J(w;X,y)\n",
        "$$\n",
        "\n",
        "We can see that the addition of the weight decay term  has modified the learning rule to multiplicatively shrink the weight vector by a constant factor on each step, just before performing the usual gradient update. THis describes what happens in a single step\n",
        "\n",
        "We will further simplify analysis by making a quadratic approximation to the objective function in the neighborhood of the value of weights tat obtian minimal unregularized training cost $w^* = argmin_w J(w)$. If the objective function is truly quadratic, as in the case of fitting a linear regression model with mean equared error, then the approximation is perfect. The approximation $\\hat{J}$ is given by\n",
        "\n",
        "$$\n",
        "  \\hat{J}(\\theta) = J(w^*) + \\frac{1}{2}(w - w^*)^\\top H(w - w^*)\n",
        "$$\n",
        "\n",
        "where $H$ is the Hessian matrix of $J$ with respect to $w$ evaluated at $w^*$. There is no first-order term in the quadratic approximation, becuase $w^*$ is defined to be a minimum, where the gradient vanishes. Likewise, because $w^*$ is the location of a minimum of $J$, we can conclude $H$ is positive semidefinite.\n",
        "\n",
        "Remember: Hessian is the Jacobian of the gradients. And positive semidefinite means the eigenvalues are zero or positive.\n",
        "\n",
        "The minimum of $\\hat{J}$ occurs where its gradient\n",
        "\n",
        "$$\n",
        "  \\nabla_w \\hat{J} (w) = H (w - w^*)\n",
        "$$\n",
        "\n",
        "is equal to $0$.\n",
        "\n",
        "To studey the effect of weight decay, we modify the equation above by adding the weight decay gradient. We can now solve for the minimum of the regularized versio of $\\hat{J}$. We use the variable $\\tilde{w}$ to represent the location of the minimum.\n",
        "\n",
        "$$\n",
        "  \\alpha \\tilde{w} + H(\\tilde{w} - w^*) = 0 \\\\\n",
        "  (H + \\alpha I) \\tilde{w} = Hw^* \\\\\n",
        "  \\tilde{w} = (H + \\alpha I)^{-1} Hw^*\n",
        "$$\n",
        "\n",
        "As $alpha$ appproaches $0$, the regularized solution $\\tilde{w}$ approaches $w^*$. But what happens as $\\alpha$ grows. Becuase $H$ is real and symmetric, we can decompose it into a diagonal matrix $\\Lambda$ and an orthonormal basis of eigenvectors, $Q$, such that $H = Q\\Lambda Q^\\top$.\n",
        "\n",
        "Applying the decomposition, we obtain\n",
        "\n",
        "$$\n",
        "  \\tilde{w} = (Q\\Lambda Q^\\top + \\alpha I)^{-1} Q\\Lambda Q^\\top w^* \\\\\n",
        "  = [Q(\\Lambda + \\alpha I) Q^\\top]^{-1} Q\\Lambda Q^\\top w^* \\\\\n",
        "  = Q(\\Lambda + \\alpha I)^{-1} \\Lambda Q^\\top w^*\n",
        "$$\n",
        "\n",
        "We see that the effect of the weight decay is to recale $w^*$ along the axes defined by the eigenvectors of $H$. Specifically, the component $w^*$ that is aligned with the $i$th eigenvector of $J$ is recaled by a factor of $\\frac{\\lambda_i}{\\lambda_i + \\alpha}$"
      ],
      "metadata": {
        "id": "Yz1XZaEkKklM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For linear regression the cost function is the sum of squared errors:\n",
        "\n",
        "$$\n",
        "  (Xw - y)^\\top (Xw - y)\n",
        "$$\n",
        "\n",
        "When we add $L^2$ regularization, the objective function changes to\n",
        "\n",
        "$$\n",
        "  (Xw - y)^\\top (Xw - y) + \\frac{1}{2}\\alpha w^\\top w\n",
        "$$\n",
        "\n",
        "This changes the normal equations for the solution from\n",
        "\n",
        "$$\n",
        "  w = (X^\\top X)^{-1} X^\\top y\n",
        "$$\n",
        "\n",
        "to\n",
        "\n",
        "$$\n",
        "  w = (X^\\top X + \\alpha I)^{-1} X^\\top y\n",
        "$$\n",
        "\n",
        "The matrix $X^\\top X$ in the equation above is proportional to the covariance matrix $\\frac{1}{m} X^\\top X$. Using the $L^2$ regularization replaces this matrix with $(X^\\top X + \\alpha I)^{-1}$. The new matrix is the same as the original one, but with the addition of $\\alpha$ to the diagonal. The diagonal entires of this matrix correspond to the variance of each input feature. We can see that $L^2$ regularization cuases the learning algorithm to percieve the input $X$ as having higher variance, which makes it shrink the weights on features whose covariance with the output target is low compared to this added variance."
      ],
      "metadata": {
        "id": "w7PEVoHR0ern"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeLoss(Layer):\n",
        "    def __init__(self, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.alpha = Tensor(alpha, autograd=True)  # Regularization strength\n",
        "\n",
        "    def forward(self, pred, target, model):\n",
        "        # Compute the Mean Squared Error (MSE)\n",
        "        mse_loss = ((pred - target)*(pred - target)).sum(0)\n",
        "\n",
        "        # Compute the Ridge (L2) regularization term\n",
        "        ridge_loss = Tensor(0.0, autograd=True)\n",
        "        for param in model.get_parameters():\n",
        "            ridge_loss += Tensor((param.data ** 2).sum(), autograd=True)\n",
        "\n",
        "        # Combine MSE loss and Ridge regularization\n",
        "        total_loss = mse_loss + self.alpha * ridge_loss\n",
        "        return total_loss\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = 784\n",
        "output_dim = 10\n",
        "hidden_size = 50\n",
        "alpha = 0.01\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "lambda_reg = 1\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential([\n",
        "    Linear(input_dim, hidden_size),\n",
        "    Tanh(),\n",
        "    Linear(hidden_size, output_dim),\n",
        "    Sigmoid()\n",
        "])\n",
        "\n",
        "criterion = RidgeLoss(lambda_reg)\n",
        "optimizer = SGD(model.get_parameters(), alpha)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct_cnt = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        input = Tensor(batch_X, autograd=True)\n",
        "        target = Tensor(batch_y, autograd=True)\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model.forward(input)\n",
        "        loss = criterion.forward(prediction, target, model)\n",
        "        total_loss += loss.data\n",
        "\n",
        "        for i in range(len(prediction.data)):\n",
        "            pred_label = prediction.data[i]\n",
        "            true_label = batch_y[i]\n",
        "            correct_cnt += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    accuracy = correct_cnt / float(len(y_train))\n",
        "    avg_loss = total_loss / (len(X_train) / batch_size)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Accuracy: {accuracy}, Average Loss: {avg_loss}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "X_test_tensor = Tensor(X_test, autograd=True)\n",
        "y_pred = model.forward(X_test_tensor)\n",
        "test_accuracy = 0\n",
        "for i in range(len(y_pred.data)):\n",
        "    pred_label = y_pred.data[i]\n",
        "    true_label = y_test [i]\n",
        "    test_accuracy += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "test_accuracy = test_accuracy / float(len(y_test))\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "mse = ((y_pred.data - y_test) ** 2).mean()\n",
        "print(f\"Test MSE: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Btj_Xl68wmT",
        "outputId": "d0c6203d-2e48-4ad9-c458-892f8c1d6000"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Accuracy: 0.172, Average Loss: [145.19732186 146.66155448 146.24996606 145.07582911 144.70046261\n",
            " 144.75474351 142.10899354 146.37186567 145.04995393 145.93868002]\n",
            "Epoch 10, Accuracy: 0.923, Average Loss: [189.73164074 189.98932553 190.61214996 190.54446947 190.44146015\n",
            " 191.47224761 190.08108794 190.66532653 191.11186838 191.37910559]\n",
            "Epoch 20, Accuracy: 0.97, Average Loss: [225.69941715 226.00573616 226.20480046 226.24789332 226.0965314\n",
            " 226.47629805 225.9280274  226.13896085 226.49410622 226.6746213 ]\n",
            "Epoch 30, Accuracy: 0.985, Average Loss: [253.90762314 254.21236082 254.20923684 254.23488688 254.15007985\n",
            " 254.26767963 254.07437896 254.10099833 254.334221   254.59041708]\n",
            "Epoch 40, Accuracy: 0.986, Average Loss: [275.15030992 275.45460852 275.35050118 275.39297843 275.31581457\n",
            " 275.38576508 275.29010917 275.25660354 275.41434058 275.70213966]\n",
            "Epoch 50, Accuracy: 0.989, Average Loss: [291.36796916 291.67199391 291.53422123 291.55827642 291.47153792\n",
            " 291.55355456 291.49512191 291.43955677 291.56711352 291.83290921]\n",
            "Epoch 60, Accuracy: 0.989, Average Loss: [304.4996997  304.80213826 304.64796067 304.65323308 304.55085186\n",
            " 304.65795462 304.61983186 304.55051395 304.66412441 304.90168226]\n",
            "Epoch 70, Accuracy: 0.989, Average Loss: [315.26923808 315.57064501 315.40567032 315.40486896 315.30306343\n",
            " 315.40870227 315.38498548 315.30713186 315.40850193 315.63843919]\n",
            "Epoch 80, Accuracy: 0.991, Average Loss: [324.46484223 324.76680825 324.59378602 324.58734789 324.48979431\n",
            " 324.57466227 324.57790281 324.49505888 324.54843643 324.81643848]\n",
            "Epoch 90, Accuracy: 0.992, Average Loss: [332.8772935  333.17828996 333.00082922 332.94702078 332.89685632\n",
            " 332.93443947 332.98844612 332.90138017 332.92319373 333.21868674]\n",
            "Test Accuracy: 0.865\n",
            "Test MSE: 0.02231294322039595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $L^1$ Regularization - Lasso regression\n",
        "\n",
        "While $L^2$ weight decay is the most common form of weight decay, there are other ways to penalize the size of model parameters. Another option is to use the $L^1$ regularization, otherwise known as **lasso regression**.\n",
        "\n",
        "Formally, $L^1$ regularization on the model parameter $w$ is defined as\n",
        "\n",
        "$$\n",
        "  \\Omega(\\theta) = ||w||_1 = \\sum_i |w_i|\n",
        "$$\n",
        "\n",
        "that is, as the sum of absolute values of the individual parameters.\n",
        "\n",
        "As with $L^2$ weight decay, $L^1$ weight decay controls the strength of the regularization by scaling the penalty $\\Omega$ using a positive hyperparameter $\\alpha$. Thus, the regularized objective function $\\tilde{J} (w;X,y)$ is given by\n",
        "\n",
        "$$\n",
        "  \\tilde{J} (w;X,y) = \\alpha ||w||_1 + J(w;X,y)\n",
        "$$\n",
        "\n",
        "with the corresponding gradient\n",
        "\n",
        "$$\n",
        "  \\nabla_w \\tilde{J} (w;X,y) = \\alpha sign(w) + \\nabla_w J(X,y;w)\n",
        "$$\n",
        "\n",
        "where $sign(w)$ is simply the sign of $w$ applied element-wise.\n",
        "\n",
        "We can see that the regularization contribution to the gradient no longer scales linearly with each $w_i$; instead it is a constant factor with sign equal to $sign(w_i)$. One consequence of this form of gradient is that we will not necessarily see clean algebraic solutions to quadratic approximiations of $J(X,y;w)$ as we did for $L^2$ regularization.\n",
        "\n",
        "Our simple linear model has a quadratic cost function that we can represent via its Taylor series. Alternatively, we could imagine that this is a truncated Taylor series approximating the cost function of a more sophisticated model. The gradient in this setting is given by\n",
        "\n",
        "$$\n",
        "  \\nabla_w \\hat{J}(w) = H(w - w^*)\n",
        "$$\n",
        "\n",
        "where, again, $H$ is a Hessian matrix of $J$ with respect to $w$ evaluated at $w^*$\n",
        "\n",
        "Becuase the $L^1$ penalty does not admit clean algebraic expressions in the case of a fully general Hessian, we will also make the further simplifying assumption that the Hessian is diagonal, $H = diag([H_{1,1}, \\cdots, H_{n,n}])$ where $H_{i,i} > 0$. This assumption holds if the data for the linear regression problem has been preprocessed to remove all correlation between input features, which is accomplished using PCA.\n",
        "\n",
        "Our quadratic approximation of the $L^1$ regularizaed objective  function decomposes into a sum over the parameters.\n",
        "\n",
        "$$\n",
        "  \\hat{J}(w;X,y) - J(w^*;x,y) + \\sum_i [\\frac{1}{2} H_{i,i}(w_i - w_i^*)^2 + \\alpha |w_i|]\n",
        "$$\n",
        "\n",
        "The problem of minmizing the cost function has an analytical solution with the following form:\n",
        "\n",
        "$$\n",
        "  w_i = sign(w^*) max \\{ |w_i| - \\frac{\\alpha}{H_{i,i}}, 0\\}\n",
        "$$\n",
        "\n",
        "Consider the situation where $w_i^* > 0$ for all $i$. There are two possible outcomes:\n",
        "\n",
        "1. The case where $w_i^* \\le \\frac{\\alpha}{H_{i,i}}$. Here the optimal value of $w_i$ under the regularized objective is simply $w_i = 0$. This occurs because the contribution of $J(w;X,y)$ to the regularized objective $\\hat{J}(w;X,y)$ is overwhelmed - in direction $i$ - by the $L^1$ regularization, which puhses the value of $w_i$ to zero.\n",
        "2.  The case where $w_i^* > \\frac{\\alpha}{H_{i,i}}$. In this case, the regularization does not move the potimal value of $w_i$ to zero but instead just shifts it in the direction by a distance equal to $\\frac{\\alpha}{H_{i,i}}$\n",
        "\n",
        "A similar process happens when $w^*_i < 0$, but with the $L^1$ penalty making $w_i$ less negative by $\\frac{\\alpha}{H_{i,i}}$, or 0.\n",
        "\n",
        "In comparison to $L^2$ regularization, $L^1$ regularization results in a solution that is more **sparse**. Spartisity in this context refers to the fact that some parameters have an optimial value of zero. The sparsity of $L^1$ regularization is a qualitatively different behaviour than arises with $L^2$ regularization.\n",
        "\n",
        "This equation gave the solution for $\\tilde{w} for $L^2$ reguarlization:\n",
        "\n",
        "$$\n",
        "  \\tilde{w} = (H + \\alpha I)^{-1} Hw^*\n",
        "$$\n",
        "\n",
        "If we revisit that equation using the assumption of diagonal and positive semidefinite Hessian $H$ that we introduced for our analysis of $L^1$ regularization, we find that\n",
        "\n",
        "$$\n",
        "   \\tilde{w} = \\frac{H_{i,i}}{H_{i,i} + \\alpha} w_i^*\n",
        "$$\n",
        "\n",
        "If $w_i$ is nonzero, then $\\tilde{w}_i$ remains nonzero. This demonstrates that $L^2$ regularization does not cause the parameters to become spare, while $L^1$ regularization may do so for large enough $\\alpha$.\n",
        "\n",
        "$L^2$ regualirzation is equivalent to MAP Bayesian inference with a Gaussian prior on the weights. For $L^1$ regularization the penalty $ \\alpha \\Omega (w) = \\alpha \\sum_i |w_i| $ used to regularize the cost function is equivalent to the log-prior term that is maximized by MAP Bayesian inference when the prior is an isotropic Laplace distribution over $w \\in \\mathbb{R}^n$:\n",
        "\n",
        "$$\n",
        "  log p(w) = \\sum_i \\log Laplace(w_i;0,\\frac{1}{a}) \\\\\n",
        "  = -a ||w||_1 + n \\log \\alpha - n \\log 2\n",
        "$$\n",
        "\n",
        "From the point of view of learning via maximization with respect to $w$, we can ignore $\\log \\alpha - \\log 2$ terms because they do not depend on $w$.\n",
        "\n",
        "Remember: An even simpler version is the **isotropic** Gaussian distribution, whose covariance matrix is a scalar times the identity matrix."
      ],
      "metadata": {
        "id": "CNZAC9McGhl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LassoLoss(Layer):\n",
        "    def __init__(self, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.alpha = Tensor(alpha, autograd=True)  # Regularization strength\n",
        "\n",
        "    def forward(self, pred, target, model):\n",
        "        # Compute the Mean Squared Error (MSE)\n",
        "        mse_loss = ((pred - target) * (pred - target)).sum(0)\n",
        "\n",
        "        # Compute the Lasso (L1) regularization term\n",
        "        lasso_loss = Tensor(0.0, autograd=True)\n",
        "        for param in model.get_parameters():\n",
        "            lasso_loss += Tensor(np.abs(param.data).sum(), autograd=True)\n",
        "\n",
        "        # Combine MSE loss and Lasso regularization\n",
        "        total_loss = mse_loss + self.alpha * lasso_loss\n",
        "        return total_loss\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = 784\n",
        "output_dim = 10\n",
        "hidden_size = 50\n",
        "alpha = 0.01\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "lambda_reg = 1\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential([\n",
        "    Linear(input_dim, hidden_size),\n",
        "    Tanh(),\n",
        "    Linear(hidden_size, output_dim),\n",
        "    Sigmoid()\n",
        "])\n",
        "\n",
        "criterion = LassoLoss(lambda_reg)\n",
        "optimizer = SGD(model.get_parameters(), alpha)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct_cnt = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        input = Tensor(batch_X, autograd=True)\n",
        "        target = Tensor(batch_y, autograd=True)\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model.forward(input)\n",
        "        loss = criterion.forward(prediction, target, model)\n",
        "        total_loss += loss.data\n",
        "\n",
        "        for i in range(len(prediction.data)):\n",
        "            pred_label = prediction.data[i]\n",
        "            true_label = batch_y[i]\n",
        "            correct_cnt += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    accuracy = correct_cnt / float(len(y_train))\n",
        "    avg_loss = total_loss / (len(X_train) / batch_size)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Accuracy: {accuracy}, Average Loss: {avg_loss}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "X_test_tensor = Tensor(X_test, autograd=True)\n",
        "y_pred = model.forward(X_test_tensor)\n",
        "test_accuracy = 0\n",
        "for i in range(len(y_pred.data)):\n",
        "    pred_label = y_pred.data[i]\n",
        "    true_label = y_test [i]\n",
        "    test_accuracy += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "test_accuracy = test_accuracy / float(len(y_test))\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "mse = ((y_pred.data - y_test) ** 2).mean()\n",
        "print(f\"Test MSE: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NmOX-myGyKa",
        "outputId": "685aea55-7dba-450f-d830-51b95ea95b0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Accuracy: 0.255, Average Loss: [1813.2890596  1817.0259701  1818.32402666 1814.35915183 1815.93382496\n",
            " 1815.31203159 1815.61792756 1818.50794285 1815.70469675 1817.93224347]\n",
            "Epoch 10, Accuracy: 0.926, Average Loss: [2000.37802281 2000.79441437 2001.38187669 2001.27622541 2001.13699321\n",
            " 2002.24642391 2000.81398097 2001.34491033 2001.89867425 2002.24468736]\n",
            "Epoch 20, Accuracy: 0.972, Average Loss: [2100.67044566 2100.95774841 2101.31908907 2101.07607834 2100.93561113\n",
            " 2101.36038848 2100.86592691 2101.12486751 2101.30972556 2101.55942621]\n",
            "Epoch 30, Accuracy: 0.981, Average Loss: [2168.91650575 2169.15234927 2169.44714888 2169.1712425  2169.12701635\n",
            " 2169.32578267 2169.10392801 2169.21285082 2169.25215003 2169.49484572]\n",
            "Epoch 40, Accuracy: 0.989, Average Loss: [2218.51948099 2218.6916636  2218.87834623 2218.69801526 2218.67455769\n",
            " 2218.87448675 2218.66108728 2218.66933018 2218.71904346 2218.97914742]\n",
            "Epoch 50, Accuracy: 0.99, Average Loss: [2255.99684624 2256.12455416 2256.23160718 2256.14165878 2256.1242552\n",
            " 2256.30084099 2256.1222542  2256.07490623 2256.10941258 2256.39171282]\n",
            "Epoch 60, Accuracy: 0.992, Average Loss: [2286.21152372 2286.3291577  2286.38540968 2286.34115246 2286.26287963\n",
            " 2286.43705266 2286.26827091 2286.26213641 2286.28418551 2286.56415484]\n",
            "Epoch 70, Accuracy: 0.995, Average Loss: [2311.06975952 2311.18333404 2311.14632482 2311.19151086 2311.10065428\n",
            " 2311.24003499 2311.0993968  2311.10699709 2311.12640282 2311.35387487]\n",
            "Epoch 80, Accuracy: 0.995, Average Loss: [2331.14947603 2331.2601773  2331.1990668  2331.26659555 2331.17233379\n",
            " 2331.2937965  2331.17055832 2331.17834888 2331.19451009 2331.40896659]\n",
            "Epoch 90, Accuracy: 0.995, Average Loss: [2347.73914349 2347.84732206 2347.7759936  2347.85282671 2347.75682162\n",
            " 2347.87106753 2347.75587661 2347.76197058 2347.77535141 2347.98562972]\n",
            "Test Accuracy: 0.865\n",
            "Test MSE: 0.021766342197387864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Norm Penalties as Constrained Optimization\n",
        "\n",
        "Consider the cost function regularized by a parameter norm penatly:\n",
        "\n",
        "$$\n",
        "  \\hat{J} (\\theta,X,y) = J(\\theta;X,y) + \\alpha \\Omega (\\theta)\n",
        "$$\n",
        "\n",
        "If we wanted to constrain $\\Omega(\\theta)$ to be less than some constant $k$, we could construct a generalized Lagrange function\n",
        "\n",
        "$$\n",
        "  \\mathcal{L} (\\theta, \\alpha; X, y) = J(\\theta;X,y) + \\alpha(\\Omega(\\theta) - k)\n",
        "$$\n",
        "\n",
        "The solution to the constrained problem is given by\n",
        "\n",
        "$$\n",
        "  \\theta^* = argmin_\\theta max_{a,a \\ge 0} \\mathcal{L}(\\theta,\\alpha)\n",
        "$$\n",
        "\n",
        "To gain some insight into the effect of this constraint, we can fix $\\alpha^*$ and view the problem as just a function of $\\theta$:\n",
        "\n",
        "$$\n",
        "  \\theta^* = arg min_\\theta \\mathcal{L}(\\theta,\\alpha^*) = arg min_\\theta J(\\theta;X,y) + \\alpha^* \\Omega(\\theta)\n",
        "$$\n",
        "\n",
        "This is exactly the same regularized training problem of minimizing $\\tilde{J}$. We can thus think of a parameter norm penalty as imposing a constraint on the weights. If $\\Omega$ is the $L^2$ norm, then the weights are constrained to lie in an $L^2$ ball. If $\\Omega$ is the $L^1$ norm, the weights are constrained to lie in a region of limited $L^1$ norm.\n",
        "\n",
        "Sometimes we may wish to use explicit constrains rather than penalties. We can modify algorithms such as stochastic gradient descent to take a step downhill on $J(\\theta)$ then project $\\theta$ back to the nearest point that satisfies $\\Omega(\\theta) < k$.\n",
        "\n",
        "(Hinton (2012) recoomend a strategy intorduced by (Srebo and Shraibman 2005): constraining the norm of each column of the weight matrix of a neural network layer, rather than constraining the Frobenious nrom of the entire weight matrix. Constraining the norm of each column seperately prevents any one hidden unit from having very large weights. If we converted with constraint into a penalty in a Lagrange function, it would be similar to $L^2$ weight decay but with a seperate KKT multiplier for the weights of each hidden unit. Each of thse KKT mutlipliers would be dynamically updated seperately to make each hidden unit obey the constraint. In practise, column norm limitation is always implmeneted as an explicit constraint with reprojection."
      ],
      "metadata": {
        "id": "u9Q5dASkpQUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization and Under-Constrained Problems\n",
        "\n",
        "In some cases, regularization is necessary for machine learning problems to be properly defined. Many linear models in machine learning, including linear regression and PCA, depend on inverting a matrix $X^top X$. This is not possible when $X^\\top X$ is singular. The matrix can be singular whenver the data-generating distribution turly has no variance in some direction, or when no variance is observed in some direction because there are fewer examples (rows of $X$) than input features (columns of $X$). In this case many forms of regularization correspond to inverting $X^top X + \\alpha I$ instead. This regularized matrix is guaranteed to be invertible.\n",
        "\n",
        "Linear problems have closed form solutions when the relevant matrix is inveritble. It is also possible for a problem with no closed form solution to be underdetermined. An example is logistic regression applied to a problem where the classes are linearly separable. If a weight vector $w$ is able to achieve perfect classification, then $2w$ will also achieve perfect classification and hihger likelihood. An iterative optmization procuded like stochastic gradient descent will continiously increase the magnitude of $w$ and, in theory, will never hault. In practise, a numerical implementation of gradient descent will eventually reach sufficiently large weights to cause numerical overflow, at which point its behaviour will depend on how the programmer has decided to handle values that are not real numbers.\n",
        "\n",
        "Most forms of regularization are able to guaranteee convergence of iterative methods applied to undetermined problems. For example, weight decay will cuase gradient descent to quit increasing the magnitude of the weights when the slope of the likelihood is equal to the weight decay coefficient.\n",
        "\n",
        "Recall that one definition of the psuedoinverse $X^+$ of matrix $X$ is\n",
        "\n",
        "$$\n",
        "   A^+ = \\lim_{\\alpha \\to 0} (X^\\top X + \\alpha I)^{-1} X^\\top\n",
        "$$\n",
        "\n",
        "We can now recognize this equation as performing linear regression with weight decay. Specifically, the equation above is the limit of\n",
        "\n",
        "$$\n",
        "  w = (X^\\top X + \\alpha I) X^\\top y\n",
        "$$\n",
        "\n",
        "as the regularization coefficient shrinks to zero. We can thus interpret the pseudoinverse as stabilizing underdetermined problems using regularization."
      ],
      "metadata": {
        "id": "f5lS3Z7kzaoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Augmentation\n",
        "\n",
        "The best way to make a machine learning model generalize better is to train it on more data. Of course, in practise, the amount of data we have is limited. One way to get around this problem is to create fake data and add it to the training set. For some machine learning tasks it is reasonably straighforward to create new fake data.\n",
        "\n",
        "Data augmentation has been a particularly effective technique for a specific classification problem: object recognition. Images are high dimensional and include an enourmous range of factors of varaition, many of which can easily be simulated. Operations like translating the training images a few pixels in each direction can often greatly improve generalization, even if the model has been designed to be partially translation invariant by using colution and poolting techniques. MAny other operations, such as rotating the image or scaling the image, have also provded quite effective.\n",
        "\n",
        "Injecting noise in the input to a neural network (Sietsma & Dow 1991) can also be seen as a form of data augmentation.\n",
        "\n",
        "Dropout (Srivastava 2014) can be seen as a process of creating new inputs by _multiplying_ by noise."
      ],
      "metadata": {
        "id": "HX1sh0Gu2nOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise Robustness\n",
        "\n",
        "Another wat that noise has been used in the service of regularizing models is by adding it to the weights. This technique has been used primarily in the context of recurrent neural networks (Jim 1996). This can be interpreted as stochastic implementation of Bayesian inference over the weights. The Bayesian treatment of learning would consider the model weights to be uncertrain and representable via a probability distribution that reflects this uncertainty. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.\n",
        "\n",
        "Noise applied to teh weights can also be interpreted as equivalent to a more traditiondal form of regularization, enoucaging stability of the function to be learned. Consider the regerssion setting, where we with to train a function $\\hat{y}(x)$ that maps a set of features $x$ to a scalar using least-squares cost function between the model predictions $\\hat{y}(x)$ and the true values of $y$:\n",
        "\n",
        "$$\n",
        "  J = \\mathbb{E}_{p(x,y)} [(\\hat{y}(x) - y)^2]\n",
        "$$\n",
        "\n",
        "The training set consists of $m$ labeled examples $\\{ (x^{(1), y^{(1)}}), \\cdots, (x^{(n)}, y^{(n)}) \\}$\n",
        "\n",
        "We can now assume that with each input presentation we also include a random pertubation $\\epsilon_w \\sim \\mathcal{N}(\\epsilon,0,\\eta I)$ of the network weights. Let us imagine that we have a standard $l$-layer MLP. We denote the perturbed model as $\\hat{y}_{\\epsilon_w}(x)$. Desipite the injection of noise, we are still interested in minimizing the squared error of the output of the network. The objective function thus becomes\n",
        "\n",
        "$$\n",
        "  \\hat{J}_w  = \\mathbb{E}_{p(x,y,\\epsilon_w)} [(\\hat{y}_{\\epsilon_w}(x) - y)^2] \\\\\n",
        "  = \\mathbb{E}_{p(x,y,\\epsilon_w)} [(\\hat{y}^2_{\\epsilon_w}(x) - 2y\\hat{y}_{\\epsilon_w} +  y^2)]\n",
        "$$\n",
        "\n",
        "For small $\\eta$, the minimization of $J$ with added weight noise (with covariance $\\eta I$) is equivalent to mininmization of $J$ with an additional regularization term: $\\eta \\mathbb{E}_{p(x,y)} [||\\nabla w \\hat{y}(x)||^2]$.\n",
        "\n",
        "This form of regularization encourages the parameters to go to regions of the parameter space where small pertubations of the weights have a relatively small influence on the output. In other words, it pushes the model into regions where the model is relatively insensitive to small variations in the weights, finding points that are not merely minima, but minima surrounded by flat regions (Hochreiter 1994).\n",
        "\n",
        "In the simplified case of linear regression (where, for instance $\\hat{y}(x) = w^\\top x + b$, this regularization term collapses into $\\eta \\mathbb{E}_{p(x)} [||x||^2]$, whicc is not a function of the parameters and therefore does not contribute to the gradient of $\\tilde{J}_W$ with resepct to the model parameters."
      ],
      "metadata": {
        "id": "IiIASXgFMmOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Injecting Noise at the Output Targets\n",
        "\n",
        "Most datasets have some number of mistakes in the $y$ labels. It can be harmful to maximize $\\log p(y|x)$ when $y$ is a mistake. One way to prevent this is to explicitly model the noise on the labels. For example we can assume that for some small constant $\\epsilon$, the training set label $y$ is correct with probability $1 - \\epsilon$, and otherwise any of the other possible labels might be correct. This assumption is easy to incorproate into the cost function analytically, rather than by explicitly drawing noise samples.\n",
        "\n",
        "For example, **label smoothing** regularizes a model based on a softmax with $k$ output values by replacing the hard 0 and 1 classficiation targets with targets of $\\frac{\\epsilon}{k-1}$ and $1 - \\epsilon$, respectively. The standard cross-entropy loss may then be used with these soft targets.\n",
        "\n",
        "Maximum likelihood learning with a softmax classifier and hard targets may actually never converge - the softmax can never predict a probability of exactly 0 or exactly 1, so it will continue to learn larger and large weights, making more extreme predictions forever. It is possible to prevent this scenario using other regularization strategies like weight decay. Label smoothing has the advantage of preventing the pursuit of hard probabilities without discouraging correct classfication.\n",
        "\n",
        "This strategy has been used since the 1980s, and continues to be featured prominently in modern neural networks (Szegedy 2016)."
      ],
      "metadata": {
        "id": "algjEecgX6Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def apply_label_smoothing(y, num_classes, smoothing=0.1):\n",
        "    return (1 - smoothing) * y + smoothing / num_classes\n",
        "\n",
        "# Set up the example\n",
        "num_classes = 5\n",
        "original_label = 2  # The correct class\n",
        "smoothing = 0.1\n",
        "\n",
        "# Create one-hot encoded label\n",
        "y_one_hot = np.zeros(num_classes)\n",
        "y_one_hot[original_label] = 1\n",
        "\n",
        "# Apply label smoothing\n",
        "y_smoothed = apply_label_smoothing(y_one_hot, num_classes, smoothing)\n",
        "\n",
        "# Set up the plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "x = np.arange(num_classes)\n",
        "\n",
        "# Plot without label smoothing\n",
        "ax1.bar(x, y_one_hot, align='center', alpha=0.8, color='blue')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([f'Class {i}' for i in range(num_classes)])\n",
        "ax1.set_ylabel('Probability')\n",
        "ax1.set_title('Without Label Smoothing')\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# Plot with label smoothing\n",
        "ax2.bar(x, y_smoothed, align='center', alpha=0.8, color='green')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels([f'Class {i}' for i in range(num_classes)])\n",
        "ax2.set_ylabel('Probability')\n",
        "ax2.set_title(f'With Label Smoothing (ε={smoothing})')\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "L8MSKU-mbhr5",
        "outputId": "ba72d1b4-a340-42b7-b98d-69e97a306af8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss(object):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if self.smoothing > 0:\n",
        "            n_classes = input.shape[1]\n",
        "            target_smooth = (1 - self.smoothing) * target.data + self.smoothing / n_classes\n",
        "            target_smooth = Tensor(target_smooth, autograd=True)\n",
        "        else:\n",
        "            target_smooth = target\n",
        "\n",
        "        temp = np.exp(input.data)\n",
        "        softmax_output = temp / np.sum(temp, axis=len(input.data.shape)-1, keepdims=True)\n",
        "        target_dist = target_smooth.data\n",
        "        loss = -(np.log(softmax_output) * target_dist).sum(axis=1).mean()\n",
        "\n",
        "        if (input.autograd):\n",
        "            out = Tensor(loss, autograd=True, creators=[input], creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.001\n",
        "epochs = 100\n",
        "batch_size = 100\n",
        "input_dim = 784\n",
        "hidden_size = 100\n",
        "output_dim = 10\n",
        "label_smoothing = 0.1  # Add this line\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential([\n",
        "    Linear(input_dim,hidden_size),\n",
        "    Tanh(),\n",
        "    Dropout(p=0.5),\n",
        "    Linear(hidden_size, hidden_size),\n",
        "    Tanh(),\n",
        "    Dropout(p=0.5),\n",
        "    Linear(hidden_size, output_dim),\n",
        "    Sigmoid()\n",
        "])\n",
        "\n",
        "criterion = CrossEntropyLoss(smoothing=label_smoothing)  # Update this line\n",
        "optimizer = SGD(model.get_parameters(), alpha)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct_cnt = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        input = Tensor(batch_X, autograd=True)\n",
        "        target = Tensor(batch_y, autograd=True)\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model.forward(input)\n",
        "        loss = criterion.forward(prediction, target)\n",
        "        total_loss += loss.data\n",
        "\n",
        "        for i in range(len(prediction.data)):\n",
        "            pred_label = prediction.data[i]\n",
        "            true_label = batch_y[i]\n",
        "            correct_cnt += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    accuracy = correct_cnt / float(len(y_train))\n",
        "    avg_loss = total_loss / (len(X_train) / batch_size)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Accuracy: {accuracy}, Average Loss: {avg_loss}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "X_test_tensor = Tensor(X_test, autograd=True)\n",
        "y_pred = model.forward(X_test_tensor)\n",
        "test_accuracy = 0\n",
        "for i in range(len(y_pred.data)):\n",
        "    pred_label = y_pred.data[i]\n",
        "    true_label = y_test [i]\n",
        "    test_accuracy += int(np.argmax(pred_label.data) == np.argmax(true_label))\n",
        "test_accuracy = test_accuracy / float(len(y_test))\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "mse = ((y_pred.data - y_test) ** 2).mean()\n",
        "print(f\"Test MSE: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2o1GrrEZ74M",
        "outputId": "2b4c0574-9899-4af0-87a9-7d3678a6d245"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Accuracy: 0.137, Average Loss: 2.2974508964824376\n",
            "Epoch 10, Accuracy: 0.605, Average Loss: 1.9733146075598669\n",
            "Epoch 20, Accuracy: 0.686, Average Loss: 1.8988775948430008\n",
            "Epoch 30, Accuracy: 0.725, Average Loss: 1.8592339213816096\n",
            "Epoch 40, Accuracy: 0.757, Average Loss: 1.8282040028308963\n",
            "Epoch 50, Accuracy: 0.798, Average Loss: 1.8011739005400613\n",
            "Epoch 60, Accuracy: 0.811, Average Loss: 1.7897192979859002\n",
            "Epoch 70, Accuracy: 0.819, Average Loss: 1.7742286525181252\n",
            "Epoch 80, Accuracy: 0.848, Average Loss: 1.7540542465711997\n",
            "Epoch 90, Accuracy: 0.848, Average Loss: 1.7511658423179561\n",
            "Test Accuracy: 0.822\n",
            "Test MSE: 0.06727253246333259\n"
          ]
        }
      ]
    }
  ]
}